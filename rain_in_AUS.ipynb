{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a95ccc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5120e4",
   "metadata": {},
   "source": [
    "# 1. Import the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2514be72",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"weatherAUS.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d4d1b8",
   "metadata": {},
   "source": [
    "# 2. Analyse the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7f9439",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9395ae1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_cols = df.select_dtypes('object').columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e91460",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a94a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396487c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking how many values each column have.\n",
    "\n",
    "df.count().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536d9887",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "missing_values = df.isna().sum().sort_values()\n",
    "num_missing_values = pd.DataFrame({'num_missing_values': missing_values})\n",
    "print(num_missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556554ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_percent = df.isnull().sum() * 100 / len(df)\n",
    "missing_values = pd.DataFrame({'missing_values(%)': missing_percent})\n",
    "missing_values.sort_values(by ='missing_values(%)' , ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d55ee32",
   "metadata": {},
   "source": [
    "# 3. Preprocessing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4639138b",
   "metadata": {},
   "source": [
    "### 3.1 Discard unnecessary data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad0cb71",
   "metadata": {},
   "source": [
    "Removing rows with missing \"RainToday\" or \"RainTomorrow\" values before preprocessing can be a good idea to make analysis and modeling simpler and faster. Because I set \"RainTomorrow\" as the target and the other columns should be related to precipitation records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfdb5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['RainToday', 'RainTomorrow'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ba75de",
   "metadata": {},
   "source": [
    "There are two ways to deal with missing values, either by deleting incomplete variables if there are too many data missing or by replacing these missing values with estimated value based on the other information available. So, any columns with more than 30% of missing values will be discarded and rest of the missing values will be replaced. Then before replaceing missing values of other columns, it's wise to first check for outliers to prevent causing errors while experimenting and entering data. In addition, it works better if the data is normally-distributed, while median imputation is preferable for skewed distribution. So, let's look through the result of data analysis with these considerations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afec8402",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_A = ['Sunshine', 'Evaporation', 'Cloud3pm', 'Cloud9am']\n",
    "df[numerical_A].hist(numerical_A, rwidth=0.7, figsize=(15, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dcb807",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['Sunshine', 'Evaporation', 'Cloud3pm', 'Cloud9am'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5a5180",
   "metadata": {},
   "source": [
    "### 3.2 Replace missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e91d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_B = ['MinTemp', 'MaxTemp', 'Rainfall', 'WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm', \n",
    "               'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Temp9am', 'Temp3pm']\n",
    "df[numerical_B].hist(numerical_B, rwidth=0.7, figsize=(20, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79271848",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df[numerical_B].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc29b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing values of normally-distributed columns with mean and skewed distribution with median\n",
    "\n",
    "df['MinTemp'] = df['MinTemp'].fillna(value = df['MinTemp'].mean())\n",
    "df['MaxTemp'] = df['MaxTemp'].fillna(value = df['MaxTemp'].median())\n",
    "df['Rainfall'] = df['Rainfall'].fillna(value = df['Rainfall'].median())\n",
    "df['WindGustSpeed'] = df['WindGustSpeed'].fillna(value = df['WindGustSpeed'].median())\n",
    "df['WindSpeed9am'] = df['WindSpeed9am'].fillna(value = df['WindSpeed9am'].median())\n",
    "df['WindSpeed3pm'] = df['WindSpeed3pm'].fillna(value = df['WindSpeed3pm'].median())\n",
    "df['Humidity9am'] = df['Humidity9am'].fillna(value = df['Humidity9am'].median())\n",
    "df['Humidity3pm'] = df['Humidity3pm'].fillna(value = df['Humidity3pm'].median())\n",
    "df['Pressure9am'] = df['Pressure9am'].fillna(value = df['Pressure9am'].median())\n",
    "df['Pressure3pm'] = df['Pressure3pm'].fillna(value = df['Pressure3pm'].median())\n",
    "df['Temp9am'] = df['Temp9am'].fillna(value = df['Temp9am'].median())\n",
    "df['Temp3pm'] = df['Temp3pm'].fillna(value = df['Temp3pm'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa9889b",
   "metadata": {},
   "source": [
    "### 3.3 Handling categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfa5bde",
   "metadata": {},
   "source": [
    "It is well known that categorical data doesn't work with machine learning and deep learning algorithms, so i encoded 'Date', 'Location', 'RainToday' and 'RainTomorrow' columns so we can predict whether or not is going to rain tomorrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888db4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = (df.dtypes == \"object\")\n",
    "object_cols = list(s[s].index)\n",
    "\n",
    "print(\"Categorical variables:\")\n",
    "print(object_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40791a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Convert 'Date' column to datetime type\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d')\n",
    "df['Year'] = df['Date'].dt.year\n",
    "df['Month'] = df['Date'].dt.month\n",
    "df['Day'] = df['Date'].dt.day\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_columns = ['Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm']\n",
    "\n",
    "for column in categorical_columns:\n",
    "    df[column] = df[column].astype('category').cat.codes\n",
    "\n",
    "# Encode 'RainToday' & 'RainTomorrow'\n",
    "df['RainToday'].replace({'No': 0, 'Yes': 1}, inplace=True)\n",
    "df['RainTomorrow'].replace({'No': 0, 'Yes': 1}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dceab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for confirming there no missing values\n",
    "\n",
    "missing_percent = df.isnull().sum() * 100 / len(df)\n",
    "missing_values = pd.DataFrame({'missing_values(%)': missing_percent})\n",
    "missing_values.sort_values(by ='missing_values(%)' , ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbf242b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066ad4cd",
   "metadata": {},
   "source": [
    "# 4. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb068c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.histogram(df, \n",
    "             x='RainToday', \n",
    "             color='RainTomorrow', \n",
    "             title='Rain Tomorrow vs. Rain Today')\n",
    "fig.update_layout(width=500, height=400, bargap=0.2)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8c05c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(df.sample(2000),\n",
    "           x='Temp9am', \n",
    "           y='Temp3pm', \n",
    "           color='RainTomorrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d3724b",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(df.sample(2000),\n",
    "           x='Humidity9am', \n",
    "           y='Humidity3pm', \n",
    "           color='RainTomorrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0a74cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(df.sample(3000),\n",
    "           x='WindGustSpeed', \n",
    "           y='Pressure3pm', \n",
    "           color='RainTomorrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381b6cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(df.sample(100000),\n",
    "           x='WindGustDir', \n",
    "           y='Rainfall', \n",
    "           color='RainTomorrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6885dcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(df.sample(10000),\n",
    "           x='WindSpeed3pm', \n",
    "           y='Pressure3pm', \n",
    "           color='RainTomorrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc6a152",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(df.sample(2000),\n",
    "           x='MaxTemp', \n",
    "           y='MinTemp', \n",
    "           color='RainTomorrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ef3b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(df.sample(2000),\n",
    "           x='Pressure9am', \n",
    "           y='Pressure3pm', \n",
    "           color='RainTomorrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f643135d",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(df.sample(2000),\n",
    "           x='Pressure9am',\n",
    "           y='Rainfall', \n",
    "           color='RainTomorrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb128f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df[['Temp9am', 'Temp3pm', 'Humidity9am', 'Humidity3pm', 'WindGustSpeed', 'MaxTemp', 'MinTemp', \n",
    "               'Rainfall', 'RainToday', 'Pressure9am', 'Pressure3pm', 'Location', 'Year', 'Month', 'Day']]\n",
    "\n",
    "target = df['RainTomorrow']\n",
    "\n",
    "features = (features - features.mean()) / features.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fdae4a",
   "metadata": {},
   "source": [
    "### 4.1 Training, Validation, and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74224c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split ratios\n",
    "split_ratio = 0.8\n",
    "\n",
    "# Split index for training\n",
    "split_index_train = int(len(features) * split_ratio)\n",
    "\n",
    "X = features.values\n",
    "y = target.values\n",
    "\n",
    "# Training and Test sets\n",
    "X_train = (X[:split_index_train] - X[:split_index_train].mean()) / X[:split_index_train].std()\n",
    "X_test = (X[split_index_train:] - X[:split_index_train].mean()) / X[:split_index_train].std()\n",
    "\n",
    "# Split index for validation (10%)\n",
    "split_index_val = int(split_index_train * 0.1)\n",
    "\n",
    "# Further split the training set into training and validation sets\n",
    "X_train, X_val = X_train[:-split_index_val], X_train[-split_index_val:]\n",
    "y_train, y_val = y[:split_index_train - split_index_val], y[split_index_train - split_index_val:split_index_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113dd148",
   "metadata": {},
   "source": [
    "### 4.2 Convert data to PyTorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734241e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y[split_index_train:], dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader for training set\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Create DataLoader for validation set\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Create DataLoader for test set\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef68fa1",
   "metadata": {},
   "source": [
    "### 4.3 Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430c9932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class LogisticNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(LogisticNeuralNetwork, self).__init__()\n",
    "\n",
    "        self.hidden_layers = nn.ModuleList([\n",
    "            nn.Linear(input_size, hidden_sizes[0]),\n",
    "            nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "            nn.Linear(hidden_sizes[1], hidden_sizes[2])\n",
    "        ])\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output_layer = nn.Linear(hidden_sizes[2], output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Forward pass through hidden layers with ReLU activation\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = self.relu(hidden_layer(x))\n",
    "\n",
    "        # Output layer with sigmoid activation\n",
    "        out = self.output_layer(x)\n",
    "        out = self.sigmoid(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "input_size = X_train.shape[1]\n",
    "hidden_sizes = [32, 16, 8]\n",
    "output_size = 1\n",
    "model = LogisticNeuralNetwork(input_size, hidden_sizes, output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb008e15",
   "metadata": {},
   "source": [
    "### 4.4 Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b8cbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function and Optimizer\n",
    "loss_fn = nn.BCELoss() # Binary Cross Entropy Loss\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b47601",
   "metadata": {},
   "source": [
    "### 4.5 Training & Validating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa21763d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Model\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, target.view(-1, 1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation during training\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "\n",
    "        for inputs, target in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, target.view(-1, 1))\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            predicted = (outputs >= 0.5).float()\n",
    "            total_val += target.size(0)\n",
    "            correct_val += (predicted == target.view(-1, 1)).sum().item()\n",
    "\n",
    "        # Test after each epoch\n",
    "        test_loss = 0\n",
    "        correct_test = 0\n",
    "        total_test = 0\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, target in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, target.view(-1, 1))\n",
    "                test_loss += loss.item()\n",
    "\n",
    "                predicted = (outputs >= 0.5).float()\n",
    "                total_test += target.size(0)\n",
    "                correct_test += (predicted == target.view(-1, 1)).sum().item()\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            val_accuracy = correct_val / total_val\n",
    "            test_accuracy = correct_test / total_test\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}, Val Loss: {val_loss/len(val_loader):.4f}, Val Accuracy: {val_accuracy:.2%}, Test Accuracy: {test_accuracy:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e4f2a8",
   "metadata": {},
   "source": [
    "# 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c96ccb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "classes = ['No Rain', 'Raining']\n",
    "\n",
    "y_pred = model(X_test)\n",
    "y_pred = y_pred.ge(0.5).view(-1).cpu()\n",
    "y_test = y_test.cpu()\n",
    "\n",
    "# Create a classification report\n",
    "report = classification_report(y_test, y_pred, target_names=classes)\n",
    "\n",
    "# Print the entire classification report\n",
    "print(report)\n",
    "\n",
    "# Extract and print the test accuracy\n",
    "test_accuracy = float(report.split()[-2])\n",
    "print(f'Test Accuracy: {test_accuracy:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1477455f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "conf_mat = confusion_matrix(y_test, y_pred, normalize='pred')\n",
    "classes = ['No Rain', 'Raining']\n",
    "df_conf_mat = pd.DataFrame(conf_mat, index=classes, columns=classes)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "heat_map = sns.heatmap(df_conf_mat, annot=True, fmt='.2%', cmap='Blues')\n",
    "heat_map.yaxis.set_ticklabels(heat_map.yaxis.get_ticklabels(), ha='right')\n",
    "heat_map.xaxis.set_ticklabels(heat_map.xaxis.get_ticklabels(), ha='right')\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.title('Confusion Matrix (% of Predictions)')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
